{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdebbb2d-017f-4771-a3ab-13f3bbc84de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "\n",
    "# Initialize Firefox Options\n",
    "firefox_options = FirefoxOptions()\n",
    "\n",
    "# Create a new Firefox Profile for your extensions\n",
    "firefox_profile = webdriver.FirefoxProfile()\n",
    "\n",
    "# Convert the profile to a base64 string and add it to the options\n",
    "firefox_options.profile = firefox_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a93393e-4cdc-461b-82b6-9d9bbdb6cedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'magnolia@12.34'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the service with GeckoDriverManager\n",
    "service = Service(GeckoDriverManager().install())\n",
    "driver = webdriver.Firefox(service=service, options=firefox_options)\n",
    "\n",
    "driver.install_addon('/Users/allensunny/Downloads/Mozilla Scraper Tools/uBlock0_1.55.0.firefox.signed.xpi',  temporary=True)\n",
    "driver.install_addon('/Users/allensunny/Downloads/Mozilla Scraper Tools/bypass_paywalls_clean-3.5.5.0.xpi', temporary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13498a62-0540-4724-a181-9cecac7cd4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Webscrpaing and request parsing \n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "\n",
    "from lxml import etree\n",
    "from lxml.html import fromstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c65f46b-0304-41a4-8a77-ffc3e12ad21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6943639d-5433-4f47-839c-22574ab1eb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allensunny/anaconda3/envs/oxford_new/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Text summarizer intialization \n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a64a11-85d8-40f9-a242-67502c6af8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the toeknizer parallizsm explicityl \n",
    "import os\n",
    "# Enable parallelism in tokenizers for improved performance\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46aea03d-cccb-4979-9308-bcaaddd7b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a71bb6a-3515-4607-8da3-bed4d836b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Druid database dump\n",
    "import pydruid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c32ed46f-614f-4b65-943a-0868a75155f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/vkbby4bd33xbf6kks2_zrswh0000gn/T/ipykernel_47284/3073400958.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#Basic packages installation\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e748dd66-7f23-4909-9b66-7adc5a0c2f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://ymcinema.com/2022/09/27/imax-goes-ai-to-deliver-better-streaming-imagery/: Status code 406\n"
     ]
    }
   ],
   "source": [
    "#Opening each tab and extracting the text within it \n",
    "Companies = ['IMAX']\n",
    "target_phrases = [\"Virtual Assistant\", \"GPT\", \"ChatGPT\", \"Virtual Assistant\", \"AI\"]\n",
    "\n",
    "results = fetch_and_summarize_company_ai_usage(Companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c6b4a0b-b7ce-4c65-a274-d25212249bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \n",
       "Name: Summary, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b23a6bb-f175-4bfb-abd0-5e986613a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_summarize_company_ai_usage(company_name):\n",
    "    \"\"\"\n",
    "    Fetch web pages related to the company's AI usage and print a summary of the content.\n",
    "    \n",
    "    :param company_name: The name of the company to search for.\n",
    "    \"\"\"\n",
    "    results = []  # List to store result dictionaries\n",
    "\n",
    "    search_query = f\"{company_name} AI Usage In within the company\"\n",
    "    search_results = search(search_query, num_results=5)  \n",
    "    \n",
    "    for url in search_results:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # This will directly raise an exception for HTTP errors\n",
    "\n",
    "            # If response is successful, process it\n",
    "            page_source = response.text\n",
    "\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 429:\n",
    "                print(\"Rate limit exceeded, waiting before retrying...\")\n",
    "                time.sleep(10)\n",
    "        \n",
    "            if response.status_code == 200:\n",
    "                page_source = response.text\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "                #Calling the summarization function\n",
    "                summarized_text = summarize_and_extract(text, target_phrases, summarizer, max_length=1000, min_length=5, do_sample=False)\n",
    "                \n",
    "                # Append result to the list\n",
    "                results.append({'URL': url, 'Summary': summarized_text, 'Company': company_name})\n",
    "\n",
    "            \n",
    "            else:\n",
    "                print(f\"Failed to retrieve {url}: Status code {response.status_code}\")\n",
    "                \n",
    "                #Here I call the selium function to run it \n",
    "                driver.get(url)\n",
    "                page_source = driver.page_source\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "                #Calling the summarization function\n",
    "                summarized_text = summarize_and_extract(text, target_phrases, summarizer, max_length=1000, min_length=5, do_sample=False)\n",
    "                \n",
    "                # Append result to the list\n",
    "                results.append({'URL': url, 'Summary': summarized_text, 'Company': company_name})\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {url}: {e}\")\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "862e4644-584f-4a93-bc19-9dc0574be3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_extract(text, target_phrases, summarizer, max_length=1000, min_length=5, do_sample=False):\n",
    "    \"\"\"\n",
    "    Summarize the given text and then extract sentences containing any of the target phrases.\n",
    "    \n",
    "    :param text: The text to summarize and extract sentences from.\n",
    "    :param target_phrases: A list of phrases to look for in the summarized text.\n",
    "    :param summarizer: The summarization function to use.\n",
    "    :param max_length: The maximum length of the summary.\n",
    "    :param min_length: The minimum length of the summary.\n",
    "    :param do_sample: Whether or not to sample in the summarization process.\n",
    "    :return: A string containing the extracted sentences from the summarized text.\n",
    "    \"\"\"\n",
    "    # Perform summarization\n",
    "    summarized = summarizer(text, max_length=max_length, min_length=min_length, do_sample=do_sample)\n",
    "    summarized_text = summarized[0]['summary_text']  # Assuming the summarizer returns a list of dictionaries\n",
    "\n",
    "    # Extract sentences containing any of the target phrases\n",
    "    extracted_sentences = [\n",
    "        sentence + '.' for sentence in summarized_text.split('. ') \n",
    "        if any(phrase in sentence for phrase in target_phrases)\n",
    "    ]\n",
    "\n",
    "    # Join extracted sentences into a cohesive summary\n",
    "    extracted_summary = \" \".join(extracted_sentences)\n",
    "\n",
    "    return extracted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b49811bf-8fdc-4682-9c13-98463824fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the companies in a loop and storing the data in a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1afe890b-a460-4ed2-9578-6f880418a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing company summary data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb6cce-f836-45a5-9280-5d3d6c0b00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_source = response.text\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "                #Calling the summarization function\n",
    "                summarized_text = summarize_and_extract(text, target_phrases, summarizer, max_length=1000, min_length=5, do_sample=False)\n",
    "                \n",
    "                # Append result to the list\n",
    "                results.append({'URL': url, 'Summary': summarized_text, 'Company': company_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5d0e1-be61-4e72-bf27-d3af5bb9ed63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587765b-aa90-4f74-94a0-816bc56b8ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24d46e-8275-4cd2-ade9-c41199434237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11863356-d126-4494-be8a-eeb65101d9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085d0ea-c0c3-4fae-9095-d6f3e6d67601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66601fb1-e2b4-44ab-b705-8c5b2390c224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b0a97-1bd2-42ee-be3a-b15b5b8b95ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2e47b-54a5-4902-85f8-f928a1943940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2c841-94b0-4cda-a224-2deb73702056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e229a2-a9ff-4140-ada1-9189dc86ed36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3b2e2-ea3f-47fb-8626-392a323dcb50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4fa3ad3-988e-47d1-accf-da5d452ffb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = 'Dow Chemicals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f16f0238-ad10-4e49-aa7e-4289253f4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []  # List to store result dictionaries\n",
    "\n",
    "search_query = f\"{company_name} AI Usage In within the company\"\n",
    "search_results = search(search_query, num_results=5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4bc035f-9609-4d71-87f8-769ea440048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://emerj.com/ai-sector-overviews/artificial-intelligence-at-dow-chemical/ due to HTTPError: Status code 403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 345. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=172)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://emerj.com/ai-sector-overviews/machine-learning-chemical-industry-basf-dow-shell/ due to HTTPError: Status code 403\n"
     ]
    }
   ],
   "source": [
    "results = []  # List to store result dictionaries\n",
    "\n",
    "for url in search_results:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # This will directly raise an exception for HTTP errors\n",
    "\n",
    "        # If response is successful, process it\n",
    "        page_source = response.text\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 429:\n",
    "            print(\"Rate limit exceeded, waiting before retrying...\")\n",
    "            time.sleep(10)\n",
    "            # Optionally, retry the request here, or skip to the next iteration\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url} due to HTTPError: Status code {e.response.status_code}\")\n",
    "            \n",
    "            #Here I call the selium function to run it \n",
    "            driver.get(url)\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "            #Calling the summarization function\n",
    "            summarized_text = summarize_and_extract(text, target_phrases, summarizer, max_length=1000, min_length=5, do_sample=False)\n",
    "            \n",
    "            # Append result to the list\n",
    "            results.append({'URL': url, 'Summary': summarized_text, 'Company': company_name})\n",
    "  \n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {url}: {e}\")\n",
    "        continue  # Move to the next URL in case of unexpected errors\n",
    "\n",
    "    # This part is outside the try-except blocks\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "    # Calling the summarization function\n",
    "    summarized_text = summarize_and_extract(text, target_phrases, summarizer, max_length=1000, min_length=5, do_sample=False)\n",
    "    \n",
    "    # Append result to the list\n",
    "    results.append({'URL': url, 'Summary': summarized_text, 'Company': company_name})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758749a0-d794-4a03-8e60-d60845af9f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb605029-ea62-4453-8bec-55932535c447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960760f-6f45-4699-9c82-10576d8ff9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
