{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdebbb2d-017f-4771-a3ab-13f3bbc84de3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779f78cf-bc86-4646-9992-26fe1e12e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "\n",
    "# Initialize Firefox Options\n",
    "firefox_options = FirefoxOptions()\n",
    "\n",
    "# Create a new Firefox Profile for your extensions\n",
    "firefox_profile = webdriver.FirefoxProfile()\n",
    "\n",
    "# Convert the profile to a base64 string and add it to the options\n",
    "firefox_options.profile = firefox_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c43447b-a3a5-49c8-a222-623cdfc1c1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/allensunny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5afcb-209d-47ab-a0ab-38547f80584a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a93393e-4cdc-461b-82b6-9d9bbdb6cedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'magnolia@12.34'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the service with GeckoDriverManager\n",
    "service = Service(GeckoDriverManager().install())\n",
    "driver = webdriver.Firefox(service=service, options=firefox_options)\n",
    "\n",
    "driver.install_addon('/Users/allensunny/Downloads/Mozilla Scraper Tools/uBlock0_1.55.0.firefox.signed.xpi',  temporary=True)\n",
    "driver.install_addon('/Users/allensunny/Downloads/Mozilla Scraper Tools/bypass_paywalls_clean-3.5.5.0.xpi', temporary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13498a62-0540-4724-a181-9cecac7cd4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Webscrpaing and request parsing \n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "\n",
    "from lxml import etree\n",
    "from lxml.html import fromstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c65f46b-0304-41a4-8a77-ffc3e12ad21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6943639d-5433-4f47-839c-22574ab1eb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allensunny/anaconda3/envs/oxford_new/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Text summarizer intialization \n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a64a11-85d8-40f9-a242-67502c6af8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the toeknizer parallizsm explicityl \n",
    "import os\n",
    "# Enable parallelism in tokenizers for improved performance\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46aea03d-cccb-4979-9308-bcaaddd7b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a71bb6a-3515-4607-8da3-bed4d836b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Druid database dump\n",
    "import pydruid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c32ed46f-614f-4b65-943a-0868a75155f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/vkbby4bd33xbf6kks2_zrswh0000gn/T/ipykernel_96901/3073400958.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#Basic packages installation\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6d303e-1d35-4955-8b32-e2c4f45ffc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63bc50b1-8efa-45d6-bc6c-7a977c94207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proxy Function extraction \n",
    "def get_random_proxy():\n",
    "    return random.choice(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5fde006-80aa-4573-8d5f-6f637f46c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consutructing the proxy farm \n",
    "with open('/Users/allensunny/Downloads/http.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "#Converting to a list of proxies to fetch data\n",
    "proxies = content.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "569181d0-fae8-4796-81ce-291031c240e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the list of companies\n",
    "with open('/Users/allensunny/Downloads/Company_list.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "#Cleaning the company names \n",
    "def clean_company_name(name):\n",
    "    \n",
    "    # Remove common corporate designations\n",
    "    cleaned_name = re.sub(r'\\s*(Inc\\.|Co\\.|Corp\\.|Corporation|Company|Incorporated|LLC|Group|Corp|Enterprises|International|Co\\.\\,|L\\.P\\.|Holdings|Inc|Co)\\.?$', '', name, flags=re.IGNORECASE)\n",
    "    return cleaned_name.strip()\n",
    "\n",
    "# Split the cleaned text into a list based on multiple spaces as a separator\n",
    "company_list = content.split('    ')  # Adjust the number of spaces based on how the original text is structured\n",
    "company_list = [company.strip() for company in company_list if company.strip()]\n",
    "\n",
    "# Further cleaning to ensure no leading/trailing spaces in company names\n",
    "company_list = [company.strip() for company in company_list]\n",
    "\n",
    "# Apply the cleaning function to each company name\n",
    "cleaned_companies = [clean_company_name(company) for company in company_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbe7c3f8-f2a4-4877-9068-e4e2e43f3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening each tab and extracting the text within it \n",
    "#Companies = ['Nike']\n",
    "target_phrases = [\"Generative AI\", \"LLM\", \"ChatGPT\", \"Llama-2\", \"BLOOMZ\", \"GPT-4\", \"Stable Diffusion 2\", \"PaLM 2\", \"Claude 2\", \"Jurrasic-2\", \"Inflection-1\", \"Titan Text\"]\n",
    "\n",
    "\n",
    "#Note that this is an issue where we cant extract the company name for the data set we have provided, probably we can do \n",
    "#Some more clever keyword extraction rather than idntifying every word \n",
    "#target_phrases = target_phrases + cleaned_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86e28e43-e602-4f0d-900b-b7a715483c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Administaff,'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_companies[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15b34997-3d0e-4107-94ea-c68e55b7e617",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.quora.com/Do-the-majority-of-companies-who-claim-to-be-using-AI-actually-do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Code has fetched the url\n",
      "In the 200 stautus code situation\n",
      "About to summarize using t5\n",
      "Done summary\n",
      "https://www.quora.com/What-is-the-role-of-AI-in-business\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Code has fetched the url\n",
      "In the 200 stautus code situation\n",
      "About to summarize using t5\n",
      "Done summary\n",
      "https://www.quora.com/What-will-be-the-impact-of-the-role-of-artificial-intelligence-in-business\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Code has fetched the url\n",
      "In the 200 stautus code situation\n",
      "About to summarize using t5\n",
      "Done summary\n",
      "https://www.quora.com/How-do-businesses-use-artificial-intelligence-and-machine-learning-for-their-data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Code has fetched the url\n",
      "In the 200 stautus code situation\n",
      "About to summarize using t5\n",
      "Done summary\n",
      "https://www.quora.com/How-is-artificial-intelligence-impacting-the-workplace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Code has fetched the url\n",
      "In the 200 stautus code situation\n",
      "About to summarize using t5\n",
      "Done summary\n",
      "https://www.quora.com/Why-would-a-company-want-to-use-artificial-intelligence-AI-What-are-some-examples-of-its-usage-in-business-and-why-it-will-be-useful-for-them\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Code has fetched the url\n",
      "In the 200 stautus code situation\n",
      "About to summarize using t5\n",
      "Done summary\n",
      "https://www.quora.com/Can-you-give-some-examples-of-companies-that-use-artificial-intelligence-for-businesses-or-jobs-How-successful-were-these-implementations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Code has fetched the url\n",
      "In the 200 stautus code situation\n",
      "About to summarize using t5\n",
      "Done summary\n",
      "https://www.quora.com/What-percentage-of-businesses-are-using-AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Code has fetched the url\n",
      "In the 200 stautus code situation\n",
      "About to summarize using t5\n",
      "Done summary\n",
      "https://www.quora.com/What-are-the-benefits-of-using-artificial-intelligence-for-businesses-Why-should-companies-focus-on-artificial-intelligence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Code has fetched the url\n",
      "In the 200 stautus code situation\n",
      "About to summarize using t5\n",
      "Done summary\n",
      "https://www.quora.com/Is-it-legal-to-use-artificial-intelligence-AI-in-your-business-without-disclosing-it-to-your-clients-customers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Code has fetched the url\n",
      "In the 200 stautus code situation\n",
      "About to summarize using t5\n",
      "Done summary\n",
      "https://www.quora.com/Can-AI-help-us-to-build-a-multi-billion-dollar-company\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Code has fetched the url\n",
      "In the 200 stautus code situation\n",
      "About to summarize using t5\n",
      "Done summary\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.quora.com/Do-the-majority-of-compa...</td>\n",
       "      <td>. .</td>\n",
       "      <td>Quora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.quora.com/What-is-the-role-of-AI-i...</td>\n",
       "      <td>. .</td>\n",
       "      <td>Quora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.quora.com/What-will-be-the-impact-...</td>\n",
       "      <td>. .</td>\n",
       "      <td>Quora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.quora.com/How-do-businesses-use-ar...</td>\n",
       "      <td>. .</td>\n",
       "      <td>Quora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.quora.com/How-is-artificial-intell...</td>\n",
       "      <td>. .</td>\n",
       "      <td>Quora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.quora.com/Why-would-a-company-want...</td>\n",
       "      <td>. .</td>\n",
       "      <td>Quora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.quora.com/Can-you-give-some-exampl...</td>\n",
       "      <td>. .</td>\n",
       "      <td>Quora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.quora.com/What-percentage-of-busin...</td>\n",
       "      <td>. .</td>\n",
       "      <td>Quora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.quora.com/What-are-the-benefits-of...</td>\n",
       "      <td>. .</td>\n",
       "      <td>Quora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.quora.com/Is-it-legal-to-use-artif...</td>\n",
       "      <td>. .</td>\n",
       "      <td>Quora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.quora.com/Can-AI-help-us-to-build-...</td>\n",
       "      <td>. .</td>\n",
       "      <td>Quora</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL Summary Company\n",
       "0   https://www.quora.com/Do-the-majority-of-compa...     . .   Quora\n",
       "1   https://www.quora.com/What-is-the-role-of-AI-i...     . .   Quora\n",
       "2   https://www.quora.com/What-will-be-the-impact-...     . .   Quora\n",
       "3   https://www.quora.com/How-do-businesses-use-ar...     . .   Quora\n",
       "4   https://www.quora.com/How-is-artificial-intell...     . .   Quora\n",
       "5   https://www.quora.com/Why-would-a-company-want...     . .   Quora\n",
       "6   https://www.quora.com/Can-you-give-some-exampl...     . .   Quora\n",
       "7   https://www.quora.com/What-percentage-of-busin...     . .   Quora\n",
       "8   https://www.quora.com/What-are-the-benefits-of...     . .   Quora\n",
       "9   https://www.quora.com/Is-it-legal-to-use-artif...     . .   Quora\n",
       "10  https://www.quora.com/Can-AI-help-us-to-build-...     . .   Quora"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "\"\"\"\n",
    "Fetch web pages related to the company's AI usage and print a summary of the content.\n",
    "\n",
    ":param company_name: The name of the company to search for.\n",
    "\"\"\"\n",
    "results = []  # List to store result dictionaries\n",
    "company_name = \"Quora\"\n",
    "\n",
    "#Create a randomization function for this \n",
    "\n",
    "#I need a time out query that cuts the execution if the the code runs above a certain time \n",
    "\n",
    "#Generate a random proxy \n",
    "proxy = get_random_proxy()\n",
    "\n",
    "#Call the search query\n",
    "search_query = f\"{company_name} AI Usage In within the company\"\n",
    "search_results = search(search_query, num_results = 10, proxy = proxy, timeout = 10)  \n",
    "\n",
    "for url in search_results:\n",
    "    \n",
    "    print(url)\n",
    "    try:\n",
    "        # Set a timeout for the request\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        # Process your response here\n",
    "        print(response.status_code)\n",
    "        print(\"Code has fetched the url\")\n",
    "\n",
    "    #Note that the timeout function isnt working for some reason, I'll have implement the multithreading logic \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Request timed out for {url}\")\n",
    "\n",
    "    #Handles all other exceptions\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        \n",
    "        # Handles other requests exceptions\n",
    "        print(f\"Request error for {url}: {e}\")\n",
    " \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response.status_code == 429:\n",
    "            print(\"Rate limit exceeded, waiting before retrying...\")\n",
    "            time.sleep(10)\n",
    "            print('Slept')\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print('In the 200 stautus code situation')\n",
    "        page_source = response.text\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        #Inital cleaning\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.decompose()  # This removes the tag and its content\n",
    "            \n",
    "        # Extract text from the modified BeautifulSoup object\n",
    "        text = soup.get_text(strip=True)\n",
    "        \n",
    "        print('About to summarize using t5')\n",
    "        \n",
    "        #Calling the summarization function\n",
    "        summarized_text = summarize_and_extract(text, target_phrases, summarizer, max_length=1000, min_length=5, do_sample=False)\n",
    "        print('Done summary')\n",
    "\n",
    "        \n",
    "        # Append result to the list\n",
    "        results.append({'URL': url, 'Summary': summarized_text, 'Company': company_name})\n",
    "\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve {url}: Status code {response.status_code}\")\n",
    "        \n",
    "        #Here I call the selium function to run it \n",
    "        #Initalize the selinium value\n",
    "        print('Starting selinium')\n",
    "        \n",
    "        driver.get(url)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        \n",
    "        #Inital cleaning\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.decompose()  # This removes the tag and its content\n",
    "            \n",
    "        # Extract text from the modified BeautifulSoup object\n",
    "        text = soup.get_text(strip=True)\n",
    "\n",
    "        #Calling the summarization function\n",
    "        summarized_text = summarize_and_extract(text, target_phrases, summarizer, max_length=1000, min_length=5, do_sample=False)\n",
    "        \n",
    "        # Append result to the list\n",
    "        results.append({'URL': url, 'Summary': summarized_text, 'Company': company_name})\n",
    "        \n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df\n",
    "#return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1212a22f-f9bd-43a1-b8e3-64044f30d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.cnet.com/tech/computing/adobe-says-its-new-generative-ai-will-help-creative-pros-not-hurt-them/\n",
      "https://www.techrepublic.com/article/gpt-4-cheat-sheet/\n",
      "https://elearning.adobe.com/2023/08/using-ai-to-create-an-interactive-scenario-with-the-all-new-adobe-captivate-part-one/\n",
      "https://community.openai.com/t/gpt-4-api-for-educational-application/571083\n",
      "https://fstoppers.com/artificial-intelligence/how-chatgpt-4-saved-me-hours-after-effects-last-night-632360\n",
      "https://www.toolify.ai/ai-news/automate-adobe-illustrator-with-gpt4-765573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Adobe Systems'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results = fetch_and_summarize_company_ai_usage(cleaned_companies[12])\n",
    "cleaned_companies[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97a8a81c-d334-4904-b034-70b63058b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "072f24a1-254d-4dd2-873e-4e72321fc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concating to the final output \n",
    "final_df = pd.concat([Results, final_df], axis = 0)\n",
    "final_df.reset_index(inplace =True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b538d8e-3ec5-492c-bfc4-27c8da2cc160",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('Companies_final_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b23a6bb-f175-4bfb-abd0-5e986613a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to take a funciton and summarize the AI usage \n",
    "def fetch_and_summarize_company_ai_usage(company_name):\n",
    "    \"\"\"\n",
    "    Fetch web pages related to the company's AI usage and print a summary of the content.\n",
    "    \n",
    "    :param company_name: The name of the company to search for.\n",
    "    \"\"\"\n",
    "    results = []  # List to store result dictionaries\n",
    "    #Generate a random proxy \n",
    "    proxy = get_random_proxy()\n",
    "    \n",
    "    search_query = f\"{company_name} Usage of GPT-4\"\n",
    "    search_results = search(search_query, num_results = 5, proxy = proxy, timeout = 10)  \n",
    "                \n",
    "    for url in search_results:\n",
    "        if url.endswith('.pdf'):\n",
    "            continue  # Skip PDF files  \n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # This will directly raise an exception for HTTP errors\n",
    "            page_source = response.text\n",
    "            \n",
    "            print(url)\n",
    "\n",
    "        #Raising the selinium exception logic\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Failed to retrieve {url}: Status code {response.status_code}\")\n",
    "            \n",
    "            #Here I call the selium function to run it \n",
    "            driver.get(url)\n",
    "            page_source = driver.page_source\n",
    "            page_source = response.text\n",
    "\n",
    "            print(url)\n",
    "            \n",
    "        #Regular runtime with the requests logic libary \n",
    "        page_source = response.text\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        #Inital cleaning\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.decompose()  # This removes the tag and its content\n",
    "            \n",
    "        # Extract text from the modified BeautifulSoup object\n",
    "        text = soup.get_text(strip=True)\n",
    "\n",
    "        #Calling the summarization function\n",
    "        summarized_text = summarize_and_extract(text, target_phrases, summarizer, max_length=50, min_length=5, do_sample=False)\n",
    "        \n",
    "        # Append result to the list\n",
    "        results.append({'URL': url, 'Summary': summarized_text, 'Company': company_name})\n",
    "    \n",
    "        results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "862e4644-584f-4a93-bc19-9dc0574be3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text keyword summarizer\n",
    "def summarize_and_extract(text, target_phrases, summarizer, max_length=100, min_length=5, do_sample=True):\n",
    "    \"\"\"\n",
    "    Summarize the given text and then extract sentences containing any of the target phrases.\n",
    "    \n",
    "    :param text: The text to summarize and extract sentences from.\n",
    "    :param target_phrases: A list of phrases to look for in the summarized text.\n",
    "    :param summarizer: The summarization function to use.\n",
    "    :param max_length: The maximum length of the summary.\n",
    "    :param min_length: The minimum length of the summary.\n",
    "    :param do_sample: Whether or not to sample in the summarization process.\n",
    "    :return: A string containing the extracted sentences from the summarized text.\n",
    "    \"\"\"\n",
    "    #Perform the text preprocessing \n",
    "    find_text = find_sentences_with_keywords(text, target_phrases)\n",
    "\n",
    "    # Join extracted sentences into a cohesive summary\n",
    "    extracted_summary = \" \".join(find_text)\n",
    "\n",
    "    # Perform summarization\n",
    "    summarized = summarizer(extracted_summary, max_length=max_length, min_length=min_length, do_sample=do_sample, early_stopping=True)\n",
    "    summarized_text = summarized[0]['summary_text']  # Assuming the summarizer returns a list of dictionariess\n",
    "\n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fce72e6c-c514-450f-bef8-ed33dd4fe67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I like how this system takes the sentences, but Iwould like to improve the process by hitting the previous and next sentences to give the system more\n",
    "#Context \n",
    "\n",
    "#Key word extraction function \n",
    "def find_sentences_with_keywords(sentence_text, keywords):\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(sentence_text)\n",
    "    \n",
    "    keyword_sentences = []\n",
    "    \n",
    "    # Loop through sentences\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # Find all keywords present as whole words in the sentence\n",
    "        found_keywords = [keyword for keyword in keywords if re.search(r'\\b' + re.escape(keyword) + r'\\b', sentence, re.IGNORECASE)]\n",
    "        \n",
    "        if found_keywords:\n",
    "            # Append sentence and found keywords to the list\n",
    "            keyword_sentences.append(sentence)\n",
    "    \n",
    "    return keyword_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6a63020-6aa8-49c8-9cbd-142ada1e1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to take a funciton and summarize the AI usage \n",
    "def Foundation_model_partners(company_name, products):\n",
    "    \"\"\"\n",
    "    Fetch web pages related to the company's AI usage and print a summary of the content.\n",
    "    \n",
    "    :param company_name: The name of the company to search for.\n",
    "    \"\"\"\n",
    "    results = []  # List to store result dictionaries\n",
    "\n",
    "    #Generate a random proxy \n",
    "    proxy = get_random_proxy()\n",
    "    \n",
    "    search_query = f\"Users or companies or partners or collaborators of {company_name} {products} list\"\n",
    "    search_results = search(search_query, num_results = 5, proxy = proxy, timeout = 10)  \n",
    "        \n",
    "    for url in search_results:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # This will directly raise an exception for HTTP errors\n",
    "            page_source = response.text\n",
    "\n",
    "            print(url)\n",
    "\n",
    "        #Raising the selinium exception logic\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"Failed to retrieve {url}: Status code {response.status_code}\")\n",
    "            \n",
    "            #Here I call the selium function to run it \n",
    "            driver.get(url)\n",
    "            page_source = driver.page_source\n",
    "            page_source = response.text\n",
    "\n",
    "            print(url)\n",
    "            \n",
    "        #Regular runtime with the requests logic libary \n",
    "        page_source = response.text\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        #Inital cleaning\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.decompose()  # This removes the tag and its content\n",
    "            \n",
    "        # Extract text from the modified BeautifulSoup object\n",
    "        text = soup.get_text(strip=True)\n",
    "\n",
    "        #Calling the summarization function\n",
    "        summarized_text = summarize_and_extract(text, target_phrases, summarizer, max_length=50, min_length=5, do_sample=False)\n",
    "        \n",
    "        # Append result to the list\n",
    "        results.append({'URL': url, 'Summary': summarized_text, 'Company': company_name})\n",
    "    \n",
    "        results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ff885c1-fc9f-47f0-87ca-5c44356579dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = \"Stability AI\"\n",
    "products = \"Stable Diffusion 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b267f68-7a68-4c30-9251-3daf4b2a2c4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stability.ai/news/stable-diffusion-v2-release\n",
      "https://github.com/Stability-AI/stablediffusion\n",
      "https://www.forbes.com/sites/kenrickcai/2023/06/04/stable-diffusion-emad-mostaque-stability-ai-exaggeration/\n",
      "https://www.assemblyai.com/blog/stable-diffusion-1-vs-2-what-you-need-to-know/\n",
      "Failed to retrieve https://www.cbinsights.com/investor/stability-ai: Status code 403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 50, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.cbinsights.com/investor/stability-ai\n",
      "https://techcrunch.com/2023/07/26/stability-ai-releases-its-latest-image-generating-model-stable-diffusion-xl-1-0/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 50, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stability.ai/partners\n"
     ]
    }
   ],
   "source": [
    "results_df = Foundation_model_partners(company_name, products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "82bead21-987c-4868-8ca1-1af1e8459426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stable Diffusion 2.0 Release24 NovWritten ByWe are pleased to announce the open-source release of.The originalled bychanged the nature of open source AI models and spawned hundreds of other models and innovations worldwide'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['Summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "abd28e29-ca7f-4fd5-88b8-26557b0b45f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://stability.ai/news/stable-diffusion-v2-release'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['URL'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4e201e83-79aa-4bf0-89d1-225a6ef38bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b7920d37-3e2e-4f42-bb0a-d48628be65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concating to the final output \n",
    "final_df = pd.concat([results_df, final_df], axis = 0)\n",
    "final_df.reset_index(inplace =True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e23be270-d50b-4be2-a15a-212055aece3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the final data out\n",
    "final_df.to_csv('output_final_new_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
